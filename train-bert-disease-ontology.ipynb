{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8768293,"sourceType":"datasetVersion","datasetId":5268933}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-06T11:50:59.755929Z","iopub.execute_input":"2024-07-06T11:50:59.756187Z","iopub.status.idle":"2024-07-06T11:51:38.814475Z","shell.execute_reply.started":"2024-07-06T11:50:59.756163Z","shell.execute_reply":"2024-07-06T11:51:38.812705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom accelerate import Accelerator\nimport os\n\n# Vérifier la disponibilité du GPU\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    print(\"GPU is available and being used.\")\nelse:\n    device = torch.device('cpu')\n    print(\"GPU is not available, using CPU.\")\n\n# Charger le tokenizer et le modèle BERT pour le MLM\nmodel_checkpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForMaskedLM.from_pretrained(model_checkpoint).to(device)  # Allouer le modèle au GPU\n\n# Charger le fichier texte\nadditional_texts = []\nwith open('/kaggle/input/ccccccc/concatenated_text_file (2).txt', 'r', encoding='utf-8') as file:  # Remplacez '/kaggle/input/votre_fichier.txt' par le chemin de votre fichier\n    for line in file:\n        additional_texts.append(line.strip())\n\n# Convertir en Dataset\nadditional_dataset = Dataset.from_dict({\"text\": additional_texts})\n\n# Tokeniser le dataset additionnel\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)  # Réduire max_length à 128\n\ntokenized_additional_dataset = additional_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n\n# Préparer le collateur de données pour MLM\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\n# Définir le chemin de sauvegarde des checkpoints et des modèles\n\n\n# Définir les arguments de formation avec Kaggle comme chemin de sauvegarde\ntraining_args = TrainingArguments(\n     output_dir=\"/kaggle/working/diseaseBert\",\n    overwrite_output_dir=True,\n    num_train_epochs=40,  # Utiliser un grand nombre d'époques pour le pré-entraînement\n    per_device_train_batch_size=8,  # Réduire la taille du lot\n    save_steps=8000,  # Sauvegarder les checkpoints toutes les 500 étapes\n    save_total_limit=2,\n    prediction_loss_only=True,\n    learning_rate=5e-5,  # Taux d'apprentissage utilisé dans BERT original\n    weight_decay=0.01,  # Décroissance de poids utilisée dans BERT original\n    fp16=True  # Utiliser le mode mixed precision\n)\n\n# Initialiser le Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_additional_dataset,\n)\n\n# Entraîner le modèle\ntrainer.train()\n\n\n!zip -r /kaggle/working/diseaseBert.zip /kaggle/working/diseaseBert","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:53:05.138152Z","iopub.execute_input":"2024-07-06T11:53:05.138906Z","iopub.status.idle":"2024-07-06T14:46:31.318745Z","shell.execute_reply.started":"2024-07-06T11:53:05.138870Z","shell.execute_reply":"2024-07-06T14:46:31.317532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-07-06T14:53:37.635770Z","iopub.execute_input":"2024-07-06T14:53:37.636132Z","iopub.status.idle":"2024-07-06T14:53:49.967722Z","shell.execute_reply.started":"2024-07-06T14:53:37.636104Z","shell.execute_reply":"2024-07-06T14:53:49.966741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T14:54:34.637010Z","iopub.execute_input":"2024-07-06T14:54:34.637744Z","iopub.status.idle":"2024-07-06T14:54:34.665843Z","shell.execute_reply.started":"2024-07-06T14:54:34.637704Z","shell.execute_reply":"2024-07-06T14:54:34.665121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T14:55:28.833892Z","iopub.execute_input":"2024-07-06T14:55:28.834627Z","iopub.status.idle":"2024-07-06T14:55:41.269217Z","shell.execute_reply.started":"2024-07-06T14:55:28.834594Z","shell.execute_reply":"2024-07-06T14:55:41.268436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}